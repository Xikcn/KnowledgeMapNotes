{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "from openai import OpenAI\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "from collections import  defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# 初始化知识图谱组件\n",
    "from OmniStore.chromadb_store import StoreTool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from KnowledgeGraphManager.KGManager import KgManager\n",
    "import leidenalg\n",
    "from igraph import Graph as IGraph\n",
    "# 基础数据结构与算法\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "\n",
    "# 图数据操作\n",
    "import networkx as nx\n",
    "from community import community_louvain  # Louvain算法库\n",
    "\n",
    "# 社区检测扩展\n",
    "import leidenalg  # Leiden算法库\n",
    "from igraph import Graph as IGraph  # 图结构转换\n",
    "\n",
    "# 语义计算\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # 余弦相似度计算\n",
    "\n",
    "# 动态权重处理\n",
    "from functools import lru_cache  # 路径缓存优化\n",
    "\n",
    "load_dotenv(dotenv_path=\"./.env\")\n",
    "\n",
    "\n",
    "device = os.getenv(\"DEVICE\")\n",
    "\n",
    "\n",
    "if os.getenv(\"IS_USE_LOCAL\") == \"True\":\n",
    "    embeddings = SentenceTransformer(\n",
    "        os.getenv(\"EMBEDDINGS_PATH\")\n",
    "    ).to(device)\n",
    "else:\n",
    "    # 初始化模型和组件\n",
    "    embeddings = SentenceTransformer(os.getenv(\"EMBEDDINGS\")).to(device)\n",
    "\n",
    "\n",
    "# 创建两个独立的存储工具\n",
    "chromadb_store = StoreTool(storage_path= os.getenv(\"CHROMADB_PATH\"), embedding_function=embeddings)\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=os.getenv(\"BASE_URL\")\n",
    ")\n",
    "\n",
    "# 多模态模型\n",
    "vl_client = OpenAI(\n",
    "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\"\n",
    "    api_key=os.getenv(\"VL_API_KEY\"),\n",
    "    base_url=os.getenv(\"VL_BASE_URL\")\n",
    ")\n",
    "from LLM.Openai_Agent import OpenaiAgent\n",
    "# 创建两个独立的agent\n",
    "rag_agent = OpenaiAgent(client)\n",
    "kg_agent = OpenaiAgent(client)\n",
    "\n",
    "# 创建两个独立的splitter\n",
    "simple_files = os.getenv(\"SIMPLE\", \"\").split(\",\")\n",
    "semantic_files = os.getenv(\"SEMANTIC\", \"\").split(\",\")\n",
    "character_files = os.getenv(\"CHARACTER\", \"\").split(\",\")\n",
    "\n",
    "# 初始化默认分割器\n",
    "kg_splitter = None\n",
    "\n",
    "# 创建默认分割器\n",
    "if len(simple_files) > 0:\n",
    "    from TextSlicer.SimpleTextSplitter import SimpleTextSplitter\n",
    "    kg_splitter = SimpleTextSplitter(2045, 1024)\n",
    "elif len(semantic_files) > 0:\n",
    "    from TextSlicer.SemanticTextSplitter import SemanticTextSplitter\n",
    "    kg_splitter = SemanticTextSplitter(2045, 1024)\n",
    "elif len(character_files) > 0:\n",
    "    from TextSlicer.CharacterTextSplitter import CharacterTextSplitter\n",
    "    kg_splitter = CharacterTextSplitter(separator=\"</end>\", keep_separator=False, max_tokens=2045, min_tokens=1024)\n",
    "\n",
    "# 创建两个独立的kg_manager\n",
    "kg_manager = KgManager(agent=kg_agent, splitter=kg_splitter, embedding_model=embeddings, store=chromadb_store)\n",
    "\n",
    "kg_manager.load_store(\"知识融合\")\n",
    "\n",
    "\n",
    "G = kg_manager.current_G\n",
    "G2 = kg_manager.current_G\n",
    "print(G)\n",
    "\n",
    "# ================== 社区划分算法部分 ==================\n",
    "# Louvain算法\n",
    "community_map = community_louvain.best_partition(G2.to_undirected())\n",
    "print(community_map, 2)\n",
    "\n",
    "# Leiden算法\n",
    "# 将networkx图转换为igraph图\n",
    "nodes = list(G2.nodes())\n",
    "node_id_map = {node: i for i, node in enumerate(nodes)}\n",
    "edges = [(node_id_map[u], node_id_map[v]) for u, v in G2.edges()]\n",
    "\n",
    "# 创建igraph图对象\n",
    "igraph_G = IGraph(directed=False)\n",
    "igraph_G.add_vertices(len(nodes))\n",
    "igraph_G.add_edges(edges)\n",
    "\n",
    "# 进行Leiden社区检测\n",
    "partition = leidenalg.find_partition(\n",
    "    igraph_G,\n",
    "    leidenalg.ModularityVertexPartition,\n",
    "    n_iterations=-1  # 使用无限迭代直到收敛\n",
    ")\n",
    "leiden_communities = partition.membership\n",
    "leiden_community_map = {node: leiden_communities[node_id_map[node]] for node in nodes}\n",
    "\n",
    "\n",
    "# ================== 可视化部分 ==================\n",
    "def visualize_communities(graph, community_dict, filename):\n",
    "    \"\"\"通用社区可视化函数\"\"\"\n",
    "    net = Network(height=\"600px\", width=\"100%\", notebook=True)\n",
    "\n",
    "    # 添加节点\n",
    "    for node in graph.nodes():\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=str(node),\n",
    "            group=community_dict[node],\n",
    "            color=\"#%06x\" % (community_dict[node] * 0x0F0F0F)\n",
    "        )\n",
    "\n",
    "    # 添加边\n",
    "    for edge in graph.edges():\n",
    "        net.add_edge(edge[0], edge[1])\n",
    "\n",
    "    # 配置物理引擎\n",
    "    net.toggle_physics(True)\n",
    "    net.show(filename)\n",
    "\n",
    "\n",
    "class CommunityRetriever:\n",
    "    def __init__(self, graph, community_map):\n",
    "        \"\"\"\n",
    "        :param graph: NetworkX 图对象（带权重）\n",
    "        :param community_map: 社区划分字典 {节点: 社区ID}\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self.community_map = community_map\n",
    "        self._build_community_index()\n",
    "\n",
    "    def _build_community_index(self):\n",
    "        \"\"\"构建社区反向索引\"\"\"\n",
    "        self.community_index = defaultdict(list)\n",
    "        for node, comm_id in self.community_map.items():\n",
    "            self.community_index[comm_id].append(node)\n",
    "\n",
    "    def find_related_entities(self, query_entities, top_n=20):\n",
    "        \"\"\"\n",
    "        带权重的社区实体检索\n",
    "        :param query_entities: 查询实体列表\n",
    "        :param top_n: 返回每个实体的相关实体数量\n",
    "        :return: 排序后的相关实体字典\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for entity in query_entities:\n",
    "            if entity not in self.graph:\n",
    "                continue\n",
    "\n",
    "            # 获取实体所属社区\n",
    "            comm_id = self.community_map.get(entity, -1)\n",
    "            community_nodes = self.community_index.get(comm_id, [])\n",
    "\n",
    "            # 计算相关性分数\n",
    "            scores = []\n",
    "            for node in community_nodes:\n",
    "                if node == entity:\n",
    "                    continue\n",
    "\n",
    "                # 计算路径权重（考虑直接连接和多跳关系）\n",
    "                try:\n",
    "                    # 获取最短路径\n",
    "                    path = nx.shortest_path(self.graph, source=entity, target=node)\n",
    "                    # 计算路径权重总和\n",
    "                    path_weight = sum(\n",
    "                        self.graph[path[i]][path[i + 1]].get('weight', 1)\n",
    "                        for i in range(len(path) - 1)\n",
    "                    )\n",
    "                    # 标准化处理：路径权重均值\n",
    "                    score = path_weight / len(path)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    score = 0\n",
    "\n",
    "                scores.append((node, score))\n",
    "\n",
    "            # 按分数降序排序\n",
    "            sorted_entities = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "            results[entity] = {\n",
    "                'community': comm_id,\n",
    "                'related': sorted_entities[:top_n]\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def enhanced_community_search(\n",
    "            self,\n",
    "            file,\n",
    "            entity_names,\n",
    "            weight_threshold=0.3,\n",
    "            top_n=20,\n",
    "            max_hops=3,\n",
    "            decay_factor=0.6,\n",
    "            semantic_weight=0.4,\n",
    "            community_boost=1.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        增强型社区知识检索（支持多跳关系、社区增强和语义融合）\n",
    "\n",
    "        Args:\n",
    "            file: 文件名\n",
    "            entity_names: 输入实体列表\n",
    "            weight_threshold: 综合权重阈值，默认0.3\n",
    "            top_n: 最大返回数量，默认20\n",
    "            max_hops: 最大关系跳数，默认3\n",
    "            decay_factor: 多跳衰减系数，默认0.6\n",
    "            semantic_weight: 语义相似度权重，默认0.4\n",
    "            community_boost: 社区增强系数，默认1.2\n",
    "\n",
    "        Returns:\n",
    "            知识库条目列表（按综合得分排序）\n",
    "        \"\"\"\n",
    "        current_G = self.get_G(file)\n",
    "        if current_G is None:\n",
    "            print(f\"无法获取知识图谱数据: {file}\")\n",
    "            return []\n",
    "\n",
    "        # 阶段1：社区检测\n",
    "        partition = community_louvain.best_partition(current_G.to_undirected())\n",
    "        community_ids = {partition[e] for e in entity_names if e in partition}\n",
    "\n",
    "        # 阶段2：构建社区子图\n",
    "        community_nodes = [n for n, cid in partition.items() if cid in community_ids]\n",
    "        subgraph = current_G.subgraph(community_nodes)\n",
    "\n",
    "        # 阶段3：多跳关系挖掘（带权重衰减）\n",
    "        path_scores = defaultdict(float)\n",
    "        for source in entity_names:\n",
    "            if source not in subgraph:\n",
    "                continue\n",
    "\n",
    "            # 带衰减的广度优先搜索\n",
    "            queue = deque([(source, 1.0, 0)])\n",
    "            visited = defaultdict(float)\n",
    "\n",
    "            while queue:\n",
    "                node, score, hops = queue.popleft()\n",
    "                if hops > max_hops:\n",
    "                    continue\n",
    "\n",
    "                # 更新路径得分（取最大值）\n",
    "                path_scores[node] = max(path_scores[node], score)\n",
    "\n",
    "                # 遍历邻居节点\n",
    "                for neighbor in subgraph[node]:\n",
    "                    edge_weight = subgraph[node][neighbor].get('weight', 0.5)\n",
    "                    new_score = score * edge_weight * (decay_factor ** hops)\n",
    "\n",
    "                    if new_score > visited[neighbor]:\n",
    "                        visited[neighbor] = new_score\n",
    "                        queue.append((neighbor, new_score, hops + 1))\n",
    "\n",
    "        # 阶段4：社区中心性计算\n",
    "        pagerank_scores = nx.pagerank(subgraph, weight='weight')\n",
    "\n",
    "        # 阶段5：语义相似度计算\n",
    "        question_text = \" \".join(entity_names)\n",
    "        question_embedding = self.embedding.encode(question_text)\n",
    "        semantic_scores = {\n",
    "            node: cosine_similarity(\n",
    "                question_embedding,\n",
    "                subgraph.nodes[node].get('embedding', np.zeros_like(question_embedding))\n",
    "            )\n",
    "            for node in path_scores.keys()\n",
    "        }\n",
    "\n",
    "        # 阶段6：动态权重融合\n",
    "        combined_scores = {}\n",
    "        for node in path_scores.keys():\n",
    "            # 基础得分组合（路径+语义）\n",
    "            base_score = (\n",
    "                    (1 - semantic_weight) * path_scores[node] +\n",
    "                    semantic_weight * semantic_scores[node]\n",
    "            )\n",
    "\n",
    "            # 社区增强\n",
    "            if node in community_nodes:\n",
    "                base_score *= community_boost\n",
    "\n",
    "            # 中心性增强\n",
    "            combined_scores[node] = (\n",
    "                    0.7 * base_score +\n",
    "                    0.3 * pagerank_scores.get(node, 0)\n",
    "            )\n",
    "\n",
    "        # 阶段7：边信息收集与筛选\n",
    "        knowledge_base = []\n",
    "        processed_pairs = set()\n",
    "\n",
    "        # 按综合得分排序节点\n",
    "        sorted_nodes = sorted(combined_scores.items(), key=lambda x: -x[1])\n",
    "\n",
    "        for node, score in sorted_nodes[:top_n * 2]:  # 扩大候选池\n",
    "            if score < weight_threshold:\n",
    "                continue\n",
    "\n",
    "            # 收集所有相关边\n",
    "            for neighbor in subgraph[node]:\n",
    "                edge_data = subgraph[node][neighbor]\n",
    "                pair = tuple(sorted([node, neighbor]))\n",
    "\n",
    "                if pair in processed_pairs:\n",
    "                    continue\n",
    "\n",
    "                processed_pairs.add(pair)\n",
    "\n",
    "                # 计算边综合得分\n",
    "                edge_score = (\n",
    "                        0.6 * score +\n",
    "                        0.3 * combined_scores.get(neighbor, 0) +\n",
    "                        0.1 * edge_data.get('weight', 0.5)\n",
    "                )\n",
    "\n",
    "                if edge_score >= weight_threshold:\n",
    "                    knowledge_base.append({\n",
    "                        'source': node,\n",
    "                        'target': neighbor,\n",
    "                        'relation': edge_data.get('label', '未知关系'),\n",
    "                        'context': edge_data.get('title', '无上下文'),\n",
    "                        'score': round(edge_score, 3),\n",
    "                        'metadata': {\n",
    "                            'path_score': round(path_scores[node], 3),\n",
    "                            'semantic_score': round(semantic_scores[node], 3),\n",
    "                            'pagerank': round(pagerank_scores.get(node, 0), 3)\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "        # 最终排序和截断\n",
    "        knowledge_base.sort(key=lambda x: -x['score'])\n",
    "        return knowledge_base[:top_n]\n",
    "\n",
    "\n",
    "# 生成Louvain可视化\n",
    "visualize_communities(G, community_map, \"louvain_community.html\")\n",
    "\n",
    "# 生成Leiden可视化\n",
    "visualize_communities(G, leiden_community_map, \"leiden_community.html\")\n",
    "\n",
    "# 初始化检索器（使用Louvain社区划分）\n",
    "louvain_retriever = CommunityRetriever(G.to_undirected(), community_map)\n",
    "\n",
    "# 初始化检索器（使用Leiden社区划分）\n",
    "leiden_retriever = CommunityRetriever(G.to_undirected(), leiden_community_map)\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设用户问题中的实体列表\n",
    "    query_entities = [\"牛顿力学\", \"星海科技\",\"经典物理\"]\n",
    "\n",
    "    # 使用Louvain算法的检索结果\n",
    "    louvain_result = louvain_retriever.find_related_entities(query_entities)\n",
    "    print(\"Louvain社区检索结果：\")\n",
    "    print(json.dumps(louvain_result, indent=5, ensure_ascii=False))\n",
    "\n",
    "    # 使用Leiden算法的检索结果\n",
    "    leiden_result = leiden_retriever.find_related_entities(query_entities)\n",
    "    print(\"\\nLeiden社区检索结果：\")\n",
    "    print(json.dumps(leiden_result, indent=5, ensure_ascii=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "阶段1：模块度优化（节点分配）\n",
    "初始化：每个节点独立为一个社区。\n",
    "局部贪婪搜索：\n",
    "遍历节点：依次将每个节点尝试移动到其邻居所在的社区。\n",
    "计算增益ΔQ：评估移动后的模块度变化。公式简化为：\n",
    "## 模块度增益公式（ΔQ）\n",
    "\n",
    "ΔQ的计算公式为：\n",
    "$$\n",
    "\\Delta Q = \\frac{k_{i,\\text{in}}}{2m} - \\frac{\\Sigma_{\\text{tot}}^c \\cdot k_i}{(2m)^2}\n",
    "$$\n",
    "\n",
    "### 符号说明：\n",
    "- $k_{i,\\text{in}}$：节点$i$与其目标社区$c$之间的边权重和。\n",
    "- $\\Sigma_{\\text{tot}}^c$：社区$c$的总边权重（包含内外边）。\n",
    "- $m$：网络总边权重（$m = \\frac{1}{2}\\sum A_{ij}$）。\n",
    "\n",
    "接受移动：若ΔQ>0，则将节点移入增益最大的社区。\n",
    "迭代调整：重复遍历所有节点直到模块度不再提升。\n",
    "阶段2：网络聚合（构建超级节点）\n",
    "合并社区：将同一社区内的节点合并为一个超级节点。\n",
    "重构网络：\n",
    "超级节点之间的边权重：原网络跨社区边的权重之和。\n",
    "超级节点内部边权重：原社区内部边的权重之和（保留自环）。\n",
    "循环迭代：将聚合后的新网络作为输入，重复阶段1和阶段2，直到模块度无法进一步优化。"
   ],
   "id": "5d68e8c023fe4ac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "模块度Q：衡量社区内实际连边与随机连边的差值之和，归一化后范围在[−0.5,1)，值越大表示社区划分越优。",
   "id": "d81ee5ab2bff3afa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "一、Louvain算法的局限性\n",
    "不连通社区：Louvain可能生成内部不连通的社区。\n",
    "局部最优陷阱：贪婪策略易陷入次优解。\n",
    "结果不稳定：节点遍历顺序影响最终划分。\n",
    "小社区遗漏：分辨率限制导致无法检测微小社区。\n",
    "二、Leiden算法的核心改进\n",
    "1. 三阶段迭代优化\n",
    "阶段1：局部节点移动（类似Louvain）。\n",
    "阶段2：子社区细化（Refinement）强制社区连通。\n",
    "阶段3：网络聚合（确保聚合后社区结构稳定）。\n",
    "2. 允许暂时性质量下降\n",
    "引入概率接受策略，以一定概率接受降低模块度的移动，逃离局部最优。\n",
    "3. 精细化社区划分\n",
    "子分区细化：将大社区递归拆分为连通子社区。\n",
    "随机性控制：通过固定随机种子提升结果可重复性。"
   ],
   "id": "479eb9bc52e377c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 阶段1：局部节点移动\n",
    "初始化：每个节点独立为社区。\n",
    "遍历节点：\n",
    "尝试将节点移动到邻居社区，计算模块度增益ΔQ。\n",
    "接受条件：ΔQ > 0 或 以概率p=exp(ΔQ/T)接受（T为退火温度参数）。\n",
    "迭代：重复直到无显著改进。\n",
    "## 阶段2：子社区细化\n",
    "连通性检验：对每个社区检测是否为连通图，若否则拆分。\n",
    "递归拆分：对不连通社区继续划分子社区，直到所有子社区连通。\n",
    "合并优化：评估子社区合并是否提升模块度。\n",
    "## 阶段3：网络聚合\n",
    "构建超级节点：将每个社区合并为超级节点。\n",
    "边权重更新：\n",
    "内部边权重：保留为自环。\n",
    "跨社区边权重：原始网络跨社区边权重之和。\n",
    "迭代：将聚合后的网络输入阶段1，直到模块度收敛。"
   ],
   "id": "25b658fab2fc3722"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "|指标\t| Louvain           | \tLeiden                    |\n",
    "|---|-------------------|----------------------------|\n",
    "|社区连通性\t| 可能产生不连通社区\t        | 强制保证社区内部连通                 |\n",
    "|结果稳定性\t| 高随机性依赖\t           | 通过细化阶段降低随机性影响              |\n",
    "|小社区检测\t| 受分辨率限制\t           | 支持更小社区的发现                  |\n",
    "|时间复杂度\t| O(n log n)\t       | O(n log n) ~ O(n²)（更精确但稍慢） |\n",
    "|适用网络规模\t| 百万级节点\t| 十万级节点（精度优先）                |"
   ],
   "id": "4ed44af77310ffa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "28e956ed44fd857a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
